{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part three attempt \n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "#nomral os stuff \n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import logging\n",
    "import codecs\n",
    "import glob\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gensim.models.word2vec",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-00a51fa19e8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Couldnt get Glove working sooooo i tried ahah\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanifold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named gensim.models.word2vec"
     ]
    }
   ],
   "source": [
    "#nltk is the best \n",
    "import nltk\n",
    "#Couldnt get Glove working sooooo i tried ahah\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filenames = sorted(glob.glob(\"query/queryall.trec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found all the things:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['query/queryall.trec']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Found all the things:\")\n",
    "data_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/butthead/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Reading 'query/queryall.trec'...\n",
      "Corpus is now 12143 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "#formating or trying to anyway \n",
    "corpus_raw = u\"\"\n",
    "for data_filename in data_filenames:\n",
    "    print(\"Reading '{0}'...\".format(data_filename))\n",
    "    with codecs.open(data_filename, \"r\", \"utf-8\") as data_file:\n",
    "        corpus_raw += data_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk is life \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trying to remove all the shit \n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_data:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'top', u'num', u'num', u'title', u'AIRBUS', u'SUBSIDIES', u'title', u'top', u'top', u'num', u'num', u'title', u'SOUTH', u'AFRICAN', u'SANCTIONS', u'title', u'top', u'top', u'num', u'num', u'title', u'LEVERAGED', u'BUYOUTS', u'title', u'top', u'top', u'num', u'num', u'title', u'SATELLITE', u'LAUNCH', u'CONTRACTS', u'title', u'top', u'top', u'num', u'num', u'title', u'INSIDER', u'TRADING', u'title', u'top', u'top', u'num', u'num', u'title', u'PRIME', u'LENDING', u'RATE', u'MOVES', u'PREDICTIONS', u'title', u'top', u'top', u'num', u'num', u'title', u'MCI', u'title', u'top', u'top', u'num', u'num', u'title', u'RAIL', u'STRIKES', u'title', u'top', u'top', u'num', u'num', u'title', u'WEATHER', u'RELATED', u'FATALITIES', u'title', u'top', u'top', u'num', u'num', u'title', u'MERIT', u'PAY', u'VS', u'SENIORITY', u'title', u'top', u'top', u'num', u'num', u'title', u'ISRAELI', u'ROLE', u'IN', u'IRAN', u'CONTRA', u'AFFAIR', u'title', u'top', u'top', u'num', u'num', u'title', u'MILITARY', u'COUPS', u'D', u'ETAT', u'title', u'top', u'top', u'num', u'num', u'title', u'MACHINE', u'TRANSLATION', u'title', u'top', u'top', u'num', u'num', u'title', u'HOSTAGE', u'TAKING', u'title', u'top', u'top', u'num', u'num', u'title', u'INFORMATION', u'RETRIEVAL', u'SYSTEMS', u'title', u'top', u'top', u'num', u'num', u'title', u'NATURAL', u'LANGUAGE', u'PROCESSING', u'title', u'top', u'top', u'num', u'num', u'title', u'POLITICALLY', u'MOTIVATED', u'CIVIL', u'DISTURBANCES', u'title', u'top', u'top', u'num', u'num', u'title', u'HEALTH', u'HAZARDS', u'FROM', u'FINE', u'DIAMETER', u'FIBERS', u'title', u'top', u'top', u'num', u'num', u'title', u'ATTEMPTS', u'TO', u'REVIVE', u'THE', u'SALT', u'II', u'TREATY', u'title', u'top', u'top', u'num', u'num', u'title', u'SURROGATE', u'MOTHERHOOD', u'title', u'top', u'top', u'num', u'num', u'title', u'BORDER', u'INCURSIONS', u'title', u'top', u'top', u'num', u'num', u'title', u'DEMOGRAPHIC', u'SHIFTS', u'IN', u'THE', u'U', u'S', u'title', u'top', u'top', u'num', u'num', u'title', u'DEMOGRAPHIC', u'SHIFTS', u'ACROSS', u'NATIONAL', u'BOUNDARIES', u'title', u'top', u'top', u'num', u'num', u'title', u'CONFLICTING', u'POLICY', u'title', u'top', u'top', u'num', u'num', u'title', u'AUTOMATION', u'title', u'top', u'top', u'num', u'num', u'title', u'U', u'S', u'CONSTITUTION', u'ORIGINAL', u'INTENT', u'title', u'top', u'top', u'num', u'num', u'title', u'POACHING', u'title', u'top', u'top', u'num', u'num', u'title', u'GREENPEACE', u'title', u'top', u'top', u'num', u'num', u'title', u'FRG', u'POLITICAL', u'PARTY', u'POSITIONS', u'title', u'top', u'top', u'num', u'num', u'title', u'PRESIDENTIAL', u'CANDIDATES', u'PLATFORMS', u'title', u'top', u'top', u'num', u'num', u'title', u'FINANCIAL', u'CRUNCH', u'FOR', u'TELEVANGELISTS', u'IN', u'THE', u'WAKE', u'OF', u'THE', u'PTLSCANDAL', u'title', u'top', u'top', u'num', u'num', u'title', u'GENETIC', u'ENGINEERING', u'title', u'top', u'top', u'num', u'num', u'title', u'MEASURES', u'TO', u'PROTECT', u'THE', u'ATMOSPHERE', u'title', u'top', u'top', u'num', u'num', u'title', u'ALTERNATIVE', u'RENEWABLE', u'ENERGY', u'PLANT', u'EQUIPMENT', u'INSTALLATION', u'title', u'top', u'top', u'num', u'num', u'title', u'OFFICIAL', u'CORRUPTION', u'title', u'top', u'top', u'num', u'num', u'title', u'BANK', u'FAILURES', u'title', u'top', u'top', u'num', u'num', u'title', u'CRIMINAL', u'ACTIONS', u'AGAINST', u'OFFICERS', u'OF', u'FAILED', u'FINANCIAL', u'INSTITUTIONS', u'title', u'top', u'top', u'num', u'num', u'title', u'CRUDE', u'OIL', u'PRICE', u'TRENDS', u'title', u'top', u'top', u'num', u'num', u'title', u'DOWNSTREAM', u'INVESTMENTS', u'BY', u'OPEC', u'MEMBER', u'STATES', u'title', u'top', u'top', u'num', u'num', u'title', u'DATA', u'ON', u'PROVEN', u'RESERVES', u'OF', u'OIL', u'NATURAL', u'GAS', u'PRODUCERS', u'title', u'top', u'top', u'num', u'num', u'title', u'U', u'S']\n"
     ]
    }
   ],
   "source": [
    "#kinda working, need to remove top, num etc\n",
    "print(sentence_to_wordlist(raw_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data corpus contains 1,690 tokens of stuff\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The data corpus contains {0:,} tokens of stuff\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features can change for hopefully better results \n",
    "num_features = 300\n",
    "\n",
    "# min word count 4 is my cut of could be 2 or 3 \n",
    "min_word_count = 3\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "#context size \n",
    "context_size = 10\n",
    "\n",
    "downsampling = 1e-3\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build im so confused\n",
    "data2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 38\n"
     ]
    }
   ],
   "source": [
    "#well thats wrong \n",
    "print(\"Word2Vec vocabulary length:\", len(data2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925, 16900)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2vec.train(sentences, total_words=token_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2vec.save(os.path.join(\"trained\", \"data2.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2vec = glove2word2vec.Word2Vec.load(os.path.join(\"trained\", \"data2.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(data2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = data2vec.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/butthead/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'FROM', 0.9997523427009583),\n",
       " (u'AGAINST', 0.999721884727478),\n",
       " (u'SATELLITE', 0.9997162818908691),\n",
       " (u'INTERNATIONAL', 0.9997023344039917),\n",
       " (u'INSIDER', 0.9996901154518127),\n",
       " (u'CONTROL', 0.9996888041496277),\n",
       " (u'ITS', 0.9996883273124695),\n",
       " (u'FOREIGN', 0.9996863603591919),\n",
       " (u'SYSTEM', 0.9996806383132935),\n",
       " (u'POLITICAL', 0.9996774792671204)]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2vec.most_similar(\"TRADING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTER is related to CONTROL, as AIDED is related to EQUIPMENT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/butthead/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'AIDED'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# could be used for query expansion \n",
    "nearest_similarity_cosmul(\"COMPUTER\", \"CONTROL\", \"EQUIPMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put different vectors files here\n",
    "with open('vectors_ap8889_cbow_s300_w5_neg20_hs0_sam1e-4_iter5.txt', 'r') as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "#Split data into list by newlines\n",
    "data = re.split('\\n', data)\n",
    "\n",
    "#make it a dictionary for ease of use\n",
    "datadict = {}\n",
    "for i in range(0,len(data)):\n",
    "   data[i] = re.split('\\s', data[i])\n",
    "   del data[i][-1]\n",
    "   datadict[data[i][0]] = data[i]\n",
    "   del datadict[data[i][0]][0]\n",
    "   \n",
    "#numbers ?\n",
    "for word in datadict:\n",
    "    for k in range(0,len(datadict[word])):\n",
    "        datadict[word][k] = float(datadict[word][k])\n",
    "\n",
    "\n",
    "            \n",
    "with open('queries.txt', 'r') as myfile:\n",
    "    querydata=myfile.read()\n",
    "#Split data into 2-D array by spaces\n",
    "for i in range(0,len(querydata)):\n",
    "    querydata[i] = re.split('\\s', querydata[i])\n",
    "    #remove all the empties\n",
    "    querydata[i] = list(filter(lambda a: a != '', querydata[i]))\n",
    "del querydata[0]\n",
    "\n",
    "\n",
    "for i in range(0,len(querydata)):\n",
    "    querybest = ''\n",
    "    #we'll only accept query extensions with a similarity above 0\n",
    "    querysimil = 0\n",
    "    dataminus = dict(datadict)\n",
    "    for j in range(0,len(querydata[i])):\n",
    "        #can set to lowercase here\n",
    "        querydata[i][j] = querydata[i][j].lower()\n",
    "        #remove because we don't want to evaluate any terms against terms already\n",
    "        #in the query\n",
    "        dataminus.pop(querydata[i][j], None)\n",
    "    for j in range(0,len(querydata[i])):\n",
    "        if querydata[i][j] in datadict:\n",
    "            for word in dataminus:\n",
    "                topside = 0\n",
    "                bottomleft = 0\n",
    "                bottomright = 0\n",
    "                for k in range(0,len(dataminus[word])):\n",
    "                    topside += datadict[querydata[i][j]][k] * dataminus[word][k]\n",
    "                    bottomleft += datadict[querydata[i][j]][k] ** 2\n",
    "                    bottomright += dataminus[word][k] ** 2\n",
    "                cosine = topside / math.sqrt(bottomleft*bottomright)\n",
    "                if (cosine > querysimil):\n",
    "                    #if cosine similarity is higher, make it the new record\n",
    "                    querybest = word\n",
    "                    querysimil = cosine\n",
    "    print(querybest)\n",
    "    querydata[i].append(querybest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
