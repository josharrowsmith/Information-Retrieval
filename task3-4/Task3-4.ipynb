{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Part three attempt \n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "#nomral os stuff \n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import logging\n",
    "import codecs\n",
    "import glob\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk is the best \n",
    "import nltk\n",
    "#Couldnt get Glove working sooooo i tried ahah\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_filenames = sorted(glob.glob(\"query/queryall.trec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found all the things:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['query/queryall.trec']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Found all the things:\")\n",
    "data_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/butthead/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Reading 'query/queryall.trec'...\n",
      "Corpus is now 12143 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "#formating or trying to anyway \n",
    "corpus_raw = u\"\"\n",
    "for data_filename in data_filenames:\n",
    "    print(\"Reading '{0}'...\".format(data_filename))\n",
    "    with codecs.open(data_filename, \"r\", \"utf-8\") as data_file:\n",
    "        corpus_raw += data_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk is life \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trying to remove all the shit \n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_data:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'top', u'num', u'num', u'title', u'AIRBUS', u'SUBSIDIES', u'title', u'top', u'top', u'num', u'num', u'title', u'SOUTH', u'AFRICAN', u'SANCTIONS', u'title', u'top', u'top', u'num', u'num', u'title', u'LEVERAGED', u'BUYOUTS', u'title', u'top', u'top', u'num', u'num', u'title', u'SATELLITE', u'LAUNCH', u'CONTRACTS', u'title', u'top', u'top', u'num', u'num', u'title', u'INSIDER', u'TRADING', u'title', u'top', u'top', u'num', u'num', u'title', u'PRIME', u'LENDING', u'RATE', u'MOVES', u'PREDICTIONS', u'title', u'top', u'top', u'num', u'num', u'title', u'MCI', u'title', u'top', u'top', u'num', u'num', u'title', u'RAIL', u'STRIKES', u'title', u'top', u'top', u'num', u'num', u'title', u'WEATHER', u'RELATED', u'FATALITIES', u'title', u'top', u'top', u'num', u'num', u'title', u'MERIT', u'PAY', u'VS', u'SENIORITY', u'title', u'top', u'top', u'num', u'num', u'title', u'ISRAELI', u'ROLE', u'IN', u'IRAN', u'CONTRA', u'AFFAIR', u'title', u'top', u'top', u'num', u'num', u'title', u'MILITARY', u'COUPS', u'D', u'ETAT', u'title', u'top', u'top', u'num', u'num', u'title', u'MACHINE', u'TRANSLATION', u'title', u'top', u'top', u'num', u'num', u'title', u'HOSTAGE', u'TAKING', u'title', u'top', u'top', u'num', u'num', u'title', u'INFORMATION', u'RETRIEVAL', u'SYSTEMS', u'title', u'top', u'top', u'num', u'num', u'title', u'NATURAL', u'LANGUAGE', u'PROCESSING', u'title', u'top', u'top', u'num', u'num', u'title', u'POLITICALLY', u'MOTIVATED', u'CIVIL', u'DISTURBANCES', u'title', u'top', u'top', u'num', u'num', u'title', u'HEALTH', u'HAZARDS', u'FROM', u'FINE', u'DIAMETER', u'FIBERS', u'title', u'top', u'top', u'num', u'num', u'title', u'ATTEMPTS', u'TO', u'REVIVE', u'THE', u'SALT', u'II', u'TREATY', u'title', u'top', u'top', u'num', u'num', u'title', u'SURROGATE', u'MOTHERHOOD', u'title', u'top', u'top', u'num', u'num', u'title', u'BORDER', u'INCURSIONS', u'title', u'top', u'top', u'num', u'num', u'title', u'DEMOGRAPHIC', u'SHIFTS', u'IN', u'THE', u'U', u'S', u'title', u'top', u'top', u'num', u'num', u'title', u'DEMOGRAPHIC', u'SHIFTS', u'ACROSS', u'NATIONAL', u'BOUNDARIES', u'title', u'top', u'top', u'num', u'num', u'title', u'CONFLICTING', u'POLICY', u'title', u'top', u'top', u'num', u'num', u'title', u'AUTOMATION', u'title', u'top', u'top', u'num', u'num', u'title', u'U', u'S', u'CONSTITUTION', u'ORIGINAL', u'INTENT', u'title', u'top', u'top', u'num', u'num', u'title', u'POACHING', u'title', u'top', u'top', u'num', u'num', u'title', u'GREENPEACE', u'title', u'top', u'top', u'num', u'num', u'title', u'FRG', u'POLITICAL', u'PARTY', u'POSITIONS', u'title', u'top', u'top', u'num', u'num', u'title', u'PRESIDENTIAL', u'CANDIDATES', u'PLATFORMS', u'title', u'top', u'top', u'num', u'num', u'title', u'FINANCIAL', u'CRUNCH', u'FOR', u'TELEVANGELISTS', u'IN', u'THE', u'WAKE', u'OF', u'THE', u'PTLSCANDAL', u'title', u'top', u'top', u'num', u'num', u'title', u'GENETIC', u'ENGINEERING', u'title', u'top', u'top', u'num', u'num', u'title', u'MEASURES', u'TO', u'PROTECT', u'THE', u'ATMOSPHERE', u'title', u'top', u'top', u'num', u'num', u'title', u'ALTERNATIVE', u'RENEWABLE', u'ENERGY', u'PLANT', u'EQUIPMENT', u'INSTALLATION', u'title', u'top', u'top', u'num', u'num', u'title', u'OFFICIAL', u'CORRUPTION', u'title', u'top', u'top', u'num', u'num', u'title', u'BANK', u'FAILURES', u'title', u'top', u'top', u'num', u'num', u'title', u'CRIMINAL', u'ACTIONS', u'AGAINST', u'OFFICERS', u'OF', u'FAILED', u'FINANCIAL', u'INSTITUTIONS', u'title', u'top', u'top', u'num', u'num', u'title', u'CRUDE', u'OIL', u'PRICE', u'TRENDS', u'title', u'top', u'top', u'num', u'num', u'title', u'DOWNSTREAM', u'INVESTMENTS', u'BY', u'OPEC', u'MEMBER', u'STATES', u'title', u'top', u'top', u'num', u'num', u'title', u'DATA', u'ON', u'PROVEN', u'RESERVES', u'OF', u'OIL', u'NATURAL', u'GAS', u'PRODUCERS', u'title', u'top', u'top', u'num', u'num', u'title', u'U', u'S']\n"
     ]
    }
   ],
   "source": [
    "#kinda working, need to remove top, num etc\n",
    "print(sentence_to_wordlist(raw_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data corpus contains 1,690 tokens of stuff\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The data corpus contains {0:,} tokens of stuff\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features can change for hopefully better results \n",
    "num_features = 300\n",
    "\n",
    "# min word count 4 is my cut of could be 2 or 3 \n",
    "min_word_count = 3\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "#context size \n",
    "context_size = 10\n",
    "\n",
    "downsampling = 1e-3\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build im so confused\n",
    "data2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 38\n"
     ]
    }
   ],
   "source": [
    "#well thats wrong \n",
    "print(\"Word2Vec vocabulary length:\", len(data2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925, 16900)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2vec.train(sentences, total_words=token_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2vec.save(os.path.join(\"trained\", \"data2.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2vec = glove2word2vec.Word2Vec.load(os.path.join(\"trained\", \"data2.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(data2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = data2vec.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/butthead/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'FROM', 0.9997523427009583),\n",
       " (u'AGAINST', 0.999721884727478),\n",
       " (u'SATELLITE', 0.9997162818908691),\n",
       " (u'INTERNATIONAL', 0.9997023344039917),\n",
       " (u'INSIDER', 0.9996901154518127),\n",
       " (u'CONTROL', 0.9996888041496277),\n",
       " (u'ITS', 0.9996883273124695),\n",
       " (u'FOREIGN', 0.9996863603591919),\n",
       " (u'SYSTEM', 0.9996806383132935),\n",
       " (u'POLITICAL', 0.9996774792671204)]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2vec.most_similar(\"TRADING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTER is related to CONTROL, as AIDED is related to EQUIPMENT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/butthead/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'AIDED'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# could be used for query expansion \n",
    "nearest_similarity_cosmul(\"COMPUTER\", \"CONTROL\", \"EQUIPMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Part four \n",
    "# need both query and raw_data\n",
    "# raw_data = all the data \n",
    "# trying to get query id\n",
    "# need the vectors\n",
    "def get_doc_id(raw):\n",
    "    clean = re.sub(\"[^1-9]\",\" \", raw)\n",
    "    ids = clean.split()\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#please work \n",
    "def tfidf(raw_data, vectores)\n",
    "    # inversed\n",
    "    tfidf_model = models.TfidfModel(raw_data, id2word=gensim_dict)\n",
    "    #comparisons\n",
    "    tfidf_index = similarities.SparseMatrixSimilarity(\n",
    "        tfidf_model[raw_data], num_features=len(gensim_dict.keys()))\n",
    "    return tfidf_model, tfidf_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define query expansion technique\n",
    "def expand_query(query, embedding, k):\n",
    "    # only expand terms in embedding vocab\n",
    "    embedding_vocab = set(list(embedding.vocab.keys()))\n",
    "    preprocess_query = query.lower().split()\n",
    "    term_expansions = []\n",
    "\n",
    "#hopefully be able to run this one day     \n",
    "def trec_run(queries, rawdict, model, index, embedd):\n",
    "    # run trec \n",
    "    run_results = dict()\n",
    "    for (k, c) in rawdict.items():\n",
    "        eq = expand_query(v, embedding, 10)  # expasion but a number\n",
    "    return SortedDict(run_results)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
